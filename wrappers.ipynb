{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b9f6ec5-85ac-4596-9c10-61dabf044789",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: langchain-core 1.0.4\n",
      "Uninstalling langchain-core-1.0.4:\n",
      "  Successfully uninstalled langchain-core-1.0.4\n",
      "Found existing installation: langchain-community 0.4.1\n",
      "Uninstalling langchain-community-0.4.1:\n",
      "  Successfully uninstalled langchain-community-0.4.1\n",
      "Found existing installation: langchain-google-genai 0.0.1\n",
      "Uninstalling langchain-google-genai-0.0.1:\n",
      "  Successfully uninstalled langchain-google-genai-0.0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Skipping langchain as it is not installed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-core\n",
      "  Using cached langchain_core-1.0.4-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting langchain-community\n",
      "  Using cached langchain_community-0.4.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-google-genai\n",
      "  Using cached langchain_google_genai-3.0.2-py3-none-any.whl.metadata (2.7 kB)\n",
      "Requirement already satisfied: google-generativeai in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (0.3.2)\n",
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.5-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-core) (1.33)\n",
      "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-core) (0.4.42)\n",
      "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-core) (25.0)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-core) (2.12.4)\n",
      "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-core) (6.0.2)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-core) (9.1.2)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-core) (4.15.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
      "Requirement already satisfied: orjson>=3.9.14 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.4)\n",
      "Requirement already satisfied: requests-toolbelt>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
      "Requirement already satisfied: requests>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.5)\n",
      "Requirement already satisfied: zstandard>=0.23.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
      "Requirement already satisfied: anyio in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (4.10.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
      "Requirement already satisfied: langchain-classic<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-community) (1.0.0)\n",
      "Requirement already satisfied: SQLAlchemy<3.0.0,>=1.4.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-community) (2.0.44)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-community) (3.13.2)\n",
      "Requirement already satisfied: dataclasses-json<0.7.0,>=0.6.7 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-community) (0.6.7)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.10.1 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-community) (2.12.0)\n",
      "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-community) (0.4.3)\n",
      "Requirement already satisfied: numpy>=1.26.2 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-community) (1.26.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.22.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from dataclasses-json<0.7.0,>=0.6.7->langchain-community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<2.0.0,>=1.0.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-classic<2.0.0,>=1.0.0->langchain-community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from pydantic-settings<3.0.0,>=2.10.1->langchain-community) (1.2.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n",
      "Requirement already satisfied: greenlet>=1 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from SQLAlchemy<3.0.0,>=1.4.0->langchain-community) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7.0,>=0.6.7->langchain-community) (1.1.0)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from langchain-google-genai) (1.2.0)\n",
      "Collecting google-ai-generativelanguage<1.0.0,>=0.7.0 (from langchain-google-genai)\n",
      "  Using cached google_ai_generativelanguage-0.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.28.1)\n",
      "Requirement already satisfied: google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (2.43.0)\n",
      "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.76.0)\n",
      "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.26.1)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (4.25.8)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.72.0)\n",
      "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0,>=1.34.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (1.62.3)\n",
      "Requirement already satisfied: cachetools<7.0,>=2.0.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (6.2.1)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (0.4.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (4.9.1)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from rsa<5,>=3.1.4->google-auth!=2.24.0,!=2.25.0,<3.0.0,>=2.14.1->google-ai-generativelanguage<1.0.0,>=0.7.0->langchain-google-genai) (0.6.1)\n",
      "INFO: pip is looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-generativeai\n",
      "  Using cached google_generativeai-0.8.4-py3-none-any.whl.metadata (4.2 kB)\n",
      "  Using cached google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.8.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.8.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.8.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.7.2-py3-none-any.whl.metadata (4.0 kB)\n",
      "  Using cached google_generativeai-0.7.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "INFO: pip is still looking at multiple versions of google-generativeai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached google_generativeai-0.7.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.6.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.5.4-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.5.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.5.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached google_generativeai-0.5.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.5.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached google_generativeai-0.4.1-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached google_generativeai-0.4.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "  Using cached google_generativeai-0.3.2-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached google_generativeai-0.3.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "  Using cached google_generativeai-0.3.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "  Using cached google_generativeai-0.2.2-py3-none-any.whl.metadata (3.1 kB)\n",
      "  Using cached google_generativeai-0.2.1-py3-none-any.whl.metadata (3.1 kB)\n",
      "  Using cached google_generativeai-0.2.0-py3-none-any.whl.metadata (3.1 kB)\n",
      "  Using cached google_generativeai-0.1.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting langchain-google-genai\n",
      "  Using cached langchain_google_genai-3.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain_google_genai-3.0.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain_google_genai-2.1.12-py3-none-any.whl.metadata (7.1 kB)\n",
      "  Using cached langchain_google_genai-2.1.11-py3-none-any.whl.metadata (6.7 kB)\n",
      "  Using cached langchain_google_genai-2.1.10-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting google-ai-generativelanguage<0.7.0,>=0.6.18 (from langchain-google-genai)\n",
      "  Using cached google_ai_generativelanguage-0.6.18-py3-none-any.whl.metadata (9.8 kB)\n",
      "INFO: pip is looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting langchain-google-genai\n",
      "  Using cached langchain_google_genai-2.1.9-py3-none-any.whl.metadata (7.2 kB)\n",
      "  Using cached langchain_google_genai-2.1.8-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached langchain_google_genai-2.1.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached langchain_google_genai-2.1.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "  Using cached langchain_google_genai-2.1.5-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached langchain_google_genai-2.1.4-py3-none-any.whl.metadata (5.2 kB)\n",
      "  Using cached langchain_google_genai-2.1.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "INFO: pip is still looking at multiple versions of langchain-google-genai to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached langchain_google_genai-2.1.2-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached langchain_google_genai-2.1.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "  Using cached langchain_google_genai-2.1.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached langchain_google_genai-2.0.11-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached langchain_google_genai-2.0.10-py3-none-any.whl.metadata (3.6 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached langchain_google_genai-2.0.9-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached langchain_google_genai-2.0.8-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached langchain_google_genai-2.0.7-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached langchain_google_genai-2.0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached langchain_google_genai-2.0.5-py3-none-any.whl.metadata (3.6 kB)\n",
      "  Using cached langchain_google_genai-2.0.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-2.0.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached langchain_google_genai-2.0.2-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached langchain_google_genai-2.0.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached langchain_google_genai-2.0.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "  Using cached langchain_google_genai-1.0.10-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-1.0.9-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-1.0.8-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-1.0.7-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-1.0.6-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-1.0.5-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-1.0.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-1.0.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-1.0.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-1.0.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-0.0.11-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Using cached langchain_google_genai-0.0.9-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached langchain_google_genai-0.0.8-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached langchain_google_genai-0.0.7-py3-none-any.whl.metadata (2.9 kB)\n",
      "  Using cached langchain_google_genai-0.0.6-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_google_genai-0.0.5-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_google_genai-0.0.4-py3-none-any.whl.metadata (2.7 kB)\n",
      "  Using cached langchain_google_genai-0.0.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "  Using cached langchain_google_genai-0.0.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "  Using cached langchain_google_genai-0.0.1-py3-none-any.whl.metadata (1.9 kB)\n",
      "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-generativeai) (0.4.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from google-generativeai) (4.67.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from anyio->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\genai-conda\\lib\\site-packages (from tqdm->google-generativeai) (0.4.6)\n",
      "Using cached langchain_core-1.0.4-py3-none-any.whl (471 kB)\n",
      "Using cached langchain_community-0.4.1-py3-none-any.whl (2.5 MB)\n",
      "Using cached langchain_google_genai-0.0.1-py3-none-any.whl (8.5 kB)\n",
      "Installing collected packages: langchain-core, langchain-google-genai, langchain-community\n",
      "\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   ---------------------------------------- 0/3 [langchain-core]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   -------------------------- ------------- 2/3 [langchain-community]\n",
      "   ---------------------------------------- 3/3 [langchain-community]\n",
      "\n",
      "Successfully installed langchain-community-0.4.1 langchain-core-1.0.4 langchain-google-genai-0.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y langchain langchain-core langchain-community langchain-google-genai\n",
    "!pip install -U langchain-core langchain-community langchain-google-genai google-generativeai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f36d82e-871a-46d2-8f98-64b7bfbc5609",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# LangChain Core\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# PDF Tools\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Google/FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8d9d5e-9677-4249-a910-40db3079412a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# 1Ô∏è‚É£ Set environment variables so all later cells can use them\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21e0ba22-0fb4-4ea4-a8af-bc68f8a3782c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GOOGLE_API_KEY set: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# If GOOGLE_API_KEY is missing but GEMINI_API_KEY exists, map it\n",
    "if \"GOOGLE_API_KEY\" not in os.environ and \"GEMINI_API_KEY\" in os.environ:\n",
    "    os.environ[\"GOOGLE_API_KEY\"] = os.environ[\"GEMINI_API_KEY\"]\n",
    "\n",
    "# Sanity check\n",
    "print(\"GOOGLE_API_KEY set:\", \"GOOGLE_API_KEY\" in os.environ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df752cc5-6632-4e55-ba4d-3dd8da43cd7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embed dim: 768\n",
      "LLM: RAG (Retrieval-Augmented Generation) is a technique that enhances LLMs by retrieving relevant information from external knowledge sources and incorporating it into the generation process.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\")\n",
    "# Use the stable versioned alias\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\", temperature=0.2, max_output_tokens=512) \n",
    "\n",
    "# Quick tests\n",
    "print(\"Embed dim:\", len(embeddings.embed_query(\"hello world\")))\n",
    "print(\"LLM:\", llm.invoke(\"In one line, what is RAG?\").content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1250177-1d97-4c4f-87b0-8a3c86ada7b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 documents (pages). Example page content head:\n",
      " Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez‚àó‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "≈Åukasz Kaiser‚àó\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin‚àó‚Ä°\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and \n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "pdf_path = \"./sample.pdf\"  # Replace with your PDF path\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()  # Returns list of Document objects (each page typically)\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents (pages). Example page content head:\\n\", docs[0].page_content[:800])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de3925c3-2dd6-4c43-b344-3cb28188a873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Deeply cleaned doc 1 (first 800 chars) ---\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@google.com Noam Shazeer‚àó Google Brain noam@google.com Niki Parmar‚àó Google Research nikip@google.com Jakob Uszkoreit‚àó Google Research usz@google.com Llion Jones‚àó Google Research llion@google.com Aidan N. Gomez‚àó‚Ä† University of Toronto aidan@cs.toronto.edu ≈Åukasz Kaiser‚àó Google Brain lukaszkaiser@google.com Illia Polosukhin‚àó‚Ä° illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and \n",
      "\n",
      "--- End preview ---\n",
      "\n",
      "\n",
      "--- Deeply cleaned doc 2 (first 800 chars) ---\n",
      "\n",
      "1 Introduction Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [3, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [3, 2, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which bec\n",
      "\n",
      "--- End preview ---\n",
      "\n",
      "‚úÖ Cleaned pages: 15\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cell ‚Äî Deep clean PDF pages with enriched metadata\n",
    "# ============================\n",
    "import re\n",
    "import unicodedata\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def improved_clean(documents, file_name):\n",
    "    cleaned_docs = []\n",
    "    for doc in documents:\n",
    "        text = doc.page_content\n",
    "        page_number = doc.metadata.get(\"page\")  # Retrieve page number from metadata\n",
    "        \n",
    "        # Dummy metadata - replace with actual extraction logic (can be improved)\n",
    "        title = \"Sample Research Paper\"  # This should be extracted from the PDF title or document header\n",
    "        authors = \"John Doe, Jane Smith\"  # Extract authors from metadata or first pages\n",
    "        publication_date = \"2024-01-01\"  # Extract publication date if available\n",
    "        source_link = \"https://arxiv.org/abs/123456\"  # If available in the document metadata\n",
    "\n",
    "        # 1) Unicode normalize (fix ligatures / odd widths)\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "        # 2) Repair hyphenation across line breaks\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1-\\2', text)\n",
    "\n",
    "        # 3) Preserve paragraph breaks\n",
    "        text = re.sub(r'\\n{2,}', '<PAR>', text)  # mark paragraphs\n",
    "        text = re.sub(r'[\\r\\n]+', ' ', text)     # flatten single newlines\n",
    "\n",
    "        # 4) Remove bracketed numeric citations like [12]\n",
    "        text = re.sub(r'\\[\\s*\\d+\\s*\\]', '', text)\n",
    "\n",
    "        # 5) Remove inline trailing citation digits glued to words (e.g., intelligence1.)\n",
    "        text = re.sub(r'(?<=\\w)(\\d{1,3})(?=[\\s\\.,;:])', '', text)\n",
    "\n",
    "        # 6) Remove long repeated digit runs (e.g., 1111, 1515151)\n",
    "        text = re.sub(r'(\\d)\\1{3,}', '', text)\n",
    "\n",
    "        # 7) Remove \"Page 12\" style markers\n",
    "        text = re.sub(r'\\bPage\\s*\\d+\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "        # 8) Strip control chars\n",
    "        text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')\n",
    "\n",
    "        # 9) Normalize whitespace and restore paragraph breaks\n",
    "        text = re.sub(r'[ \\t\\f\\v]+', ' ', text)  # collapse horizontal whitespace\n",
    "        text = text.replace('<PAR>', '\\n\\n')     # restore paragraphs\n",
    "        text = re.sub(r' {2,}', ' ', text).strip()\n",
    "\n",
    "        # 10) Targeted glyph fixes (extend if you see more)\n",
    "        replacements = {\n",
    "            'Trade-o∆Ø': 'Trade-off',\n",
    "            'Tradeo∆Ø': 'Trade-off',\n",
    "            'oe∆Ø': 'oeff',\n",
    "            'coe∆Ø': 'coeff',\n",
    "            '∆Ø': 'f',  # keep last: broadest\n",
    "        }\n",
    "        for k, v in replacements.items():\n",
    "            text = text.replace(k, v)\n",
    "\n",
    "        # Assign metadata (file_name, page number, title, authors, date, source_link)\n",
    "        doc.metadata[\"source\"] = file_name\n",
    "        doc.metadata[\"page\"] = page_number\n",
    "        doc.metadata[\"title\"] = title\n",
    "        doc.metadata[\"authors\"] = authors\n",
    "        doc.metadata[\"publication_date\"] = publication_date\n",
    "        doc.metadata[\"source_link\"] = source_link\n",
    "\n",
    "        # Determine section heading\n",
    "        if page_number in [\"0\", \"1\"]:\n",
    "            doc.metadata[\"section_heading\"] = \"authors\"\n",
    "        elif \"references\" in text.lower() or \"bibliography\" in text.lower():\n",
    "            doc.metadata[\"section_heading\"] = \"references\"\n",
    "        else:\n",
    "            doc.metadata[\"section_heading\"] = \"body\"\n",
    "\n",
    "        # Append the cleaned document with updated metadata\n",
    "        cleaned_docs.append(Document(page_content=text, metadata=doc.metadata))\n",
    "\n",
    "    return cleaned_docs\n",
    "\n",
    "# Apply to 'docs' (output of PyMuPDFLoader.load())\n",
    "docs_deeper_cleaned = improved_clean(docs, \"sample.pdf\")  # Pass the file name for metadata\n",
    "\n",
    "# Safe previews for first 2 cleaned docs\n",
    "for i, d in enumerate(docs_deeper_cleaned[:2]):\n",
    "    print(f\"\\n--- Deeply cleaned doc {i+1} (first 800 chars) ---\\n\")\n",
    "    print(d.page_content[:800])\n",
    "    print(\"\\n--- End preview ---\\n\")\n",
    "\n",
    "print(f\"‚úÖ Cleaned pages: {len(docs_deeper_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "70049445-edca-44f4-ac86-49e0c9d1fb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total chunks created: 95\n",
      "üìÑ Example chunk preview:\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@google.com Noam Shazeer‚àó Google Brain noam@google.com Niki Parmar‚àó Google Research nikip@google.com Jakob Uszkoreit‚àó Google Research usz@google.com Llion Jones‚àó Google Research llion@google.com Aidan N\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=30,\n",
    "    add_start_index=True,\n",
    "    separators=(\"\\n\\n\", \"\\n\", \". \", \" \", \"\")   # real newlines, not escaped\n",
    ")\n",
    "\n",
    "# Split pages -> chunks (metadata is auto-propagated; don‚Äôt remap it)\n",
    "chunked_docs = splitter.split_documents(docs_deeper_cleaned)\n",
    "\n",
    "# Per-chunk traceability; do NOT reassign parent metadata via modulo\n",
    "for i, doc in enumerate(chunked_docs):\n",
    "    start = doc.metadata.get(\"start_index\")\n",
    "    if start is not None:\n",
    "        doc.metadata[\"char_start\"] = start\n",
    "        doc.metadata[\"char_end\"] = start + len(doc.page_content)\n",
    "        # (optional) drop the raw start_index if you don‚Äôt need it\n",
    "        # del doc.metadata[\"start_index\"]\n",
    "\n",
    "    doc.metadata[\"chunk_id\"] = i\n",
    "    # (optional) ensure keys exist without breaking provenance\n",
    "    doc.metadata.setdefault(\"title\", \"Unknown Title\")\n",
    "    doc.metadata.setdefault(\"authors\", \"Unknown Authors\")\n",
    "    doc.metadata.setdefault(\"publication_date\", \"Unknown Date\")\n",
    "    doc.metadata.setdefault(\"section_heading\", \"Unknown Section\")\n",
    "\n",
    "print(f\"‚úÖ Total chunks created: {len(chunked_docs)}\")\n",
    "print(\"üìÑ Example chunk preview:\\n\")\n",
    "print(chunked_docs[0].page_content[:800] if chunked_docs else \"‚ö†Ô∏è No chunks produced.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "481f9d86-e1cb-4f84-8a38-12fee1eb1f98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 95 chunks to chunks.json\n",
      "üìÑ Example chunk preview:\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@google.com Noam Shazeer‚àó Google Brain noam@google.com Niki Parmar‚àó Google Research nikip@google.com Jakob Uszkoreit‚àó Google Research usz@google.com Llion Jones‚àó G\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=30,                 # match the creator cell (optional but tidy)\n",
    "    add_start_index=True,\n",
    "    separators=(\"\\n\\n\", \"\\n\", \". \", \" \", \"\"),\n",
    ")\n",
    "\n",
    "chunked_docs = splitter.split_documents(docs_deeper_cleaned)\n",
    "\n",
    "for i, doc in enumerate(chunked_docs):\n",
    "    doc.metadata[\"chunk_id\"] = i\n",
    "    start = doc.metadata.get(\"start_index\")\n",
    "    if start is not None:\n",
    "        doc.metadata[\"char_start\"] = start\n",
    "        doc.metadata[\"char_end\"] = start + len(doc.page_content)\n",
    "        # del doc.metadata[\"start_index\"]  # optional\n",
    "\n",
    "chunks_data = [{\"text\": d.page_content, \"metadata\": d.metadata} for d in chunked_docs]\n",
    "\n",
    "output_path = \"chunks.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(chunks_data)} chunks to {output_path}\")\n",
    "print(\"üìÑ Example chunk preview:\")\n",
    "print(chunks_data[0][\"text\"][:400] if chunks_data else \"No chunks.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2eedada8-847f-4281-8e7a-86cae05bac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Embeddings\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"text-embedding-004\")\n",
    "\n",
    "# 2) Vectorstore (FAISS)\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# Assume `docs` is a list[Document] (your chunked docs with metadata)\n",
    "vectorstore = FAISS.from_documents(chunked_docs, embeddings)  # default IP; cosine-equivalent with unit vectors\n",
    "\n",
    "# 3) Persist\n",
    "vectorstore.save_local(\"faiss_index\")\n",
    "\n",
    "# 4) Reload (per docs)\n",
    "reloaded = FAISS.load_local(\"faiss_index\", embeddings, allow_dangerous_deserialization=True)\n",
    "\n",
    "# 5a) Retriever (top-k)\n",
    "retriever = reloaded.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "# 5b) Retriever (thresholded; no manual scoring)\n",
    "retriever = reloaded.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 4})\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "123c143e-3095-4bfb-adab-b069e7abaa07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "1. Page 10 ‚Äî Sample Research Paper\n",
      ". Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2. Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1.10099v, 2. Yoon Kim, Carl De ...\n",
      "\n",
      "2. Page 1 ‚Äî Sample Research Paper\n",
      ". Here, the encoder maps an input sequence of symbol representations (x, ..., xn) to a sequence of continuous representations z = (z, ..., zn). Given z, the decoder then generates an output sequence (y, ..., ym) of symbols one element at a time. At each step the model is auto-regressive , consuming  ...\n",
      "\n",
      "3. Page 9 ‚Äî Sample Research Paper\n",
      ". CoRR, abs/1.0, 2. Denny Britz, Anna Goldie, Minh-Thang Luong, and Quoc V. Le. Massive exploration of neural machine translation architectures. CoRR, abs/1.03, 2. Jianpeng Cheng, Li Dong, and Mirella Lapata. Long short-term memory-networks for machine reading. arXiv preprint arXiv:1.06, 2. 10 ...\n",
      "\n",
      "4. Page 1 ‚Äî Sample Research Paper\n",
      ". Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherent ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"what is machine learning?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(\"Retrieved Documents:\")\n",
    "for i, d in enumerate(retrieved_docs, 1):\n",
    "    print(f\"{i}. Page {d.metadata.get('page', '?')} ‚Äî {d.metadata.get('title', 'Untitled')}\")\n",
    "    print(d.page_content[:300], \"...\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3af34369-b2b6-472b-94df-952e64fc1d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚úÖ RAG chain with strict citations (Gemini 2.5 Flash)\n",
    "from pathlib import Path\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n",
    "\n",
    "def fmt_docs(docs):\n",
    "    out = []\n",
    "    for d in docs:\n",
    "        src = Path(d.metadata.get(\"source\",\"\")).name or \"document.pdf\"\n",
    "        pg = d.metadata.get(\"page\",\"?\")\n",
    "        out.append(f\"[source: {src}, p. {pg}]\\n{d.page_content}\")\n",
    "    return \"\\n\\n\".join(out)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "   (\"system\",\n",
    "     # 1. High-Value Persona and Core Directive\n",
    "     \"You are an **Expert Research Scientist** specializing in technical analysis of scientific papers. Your sole purpose is to provide highly accurate, factually grounded answers to the user's question.\\n\\n\"\n",
    "     \n",
    "     # 2. Strict Grounding and Citation Rules\n",
    "     \"**CRITICAL INSTRUCTIONS:**\\n\"\n",
    "     \"1.  **STRICTLY GROUNDED:** Answer the question ONLY using the facts, findings, and figures present in the provided [CONTEXT] chunks.\\n\"\n",
    "     \"2.  **MANDATORY CITATION:** You **MUST** append an in-line citation (e.g., [source X]) immediately after every distinct fact or sentence derived from the context.\\n\"\n",
    "     \"3.  **SYNTHESIS:** Synthesize findings coherently and fluently. Do not simply list sentences. Structure your response using paragraphs and bullet points for clarity.\\n\"\n",
    "     \n",
    "     # 3. Flexible Escape Hatch\n",
    "     \"4. **FLEXIBLE ANSWER ESCAPE:** If the [CONTEXT] does not contain enough information to answer the question fully, you may respond with one of the following:\\n\"\n",
    "     \"  - If there is partial information: 'The paper provides some insights, but additional details may be needed to fully answer this question.'\\n\"\n",
    "     \"  - If no relevant information is found: 'The provided paper does not contain sufficient information to answer this question.'\\n\"\n",
    "     \"  - Always **indicate** the limitations in the answer to provide clarity about what information is available from the paper.\"\n",
    "    ),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer with strict citations.\")\n",
    "])\n",
    "\n",
    "rag = (\n",
    "    {\"context\": retriever | fmt_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "179254de-e490-4266-a726-88e9348fd278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "≈Åukasz Kaiser is listed as an author of the \"Attention Is All You Need\" paper, affiliated with Google Brain [source: sample.pdf, p. 0]. The provided context indicates his authorship, but it does not specify the particular aspects or sections of the paper he contributed to [source: sample.pdf, p. 0]. The paper provides some insights, but additional details may be needed to fully answer this question.\n"
     ]
    }
   ],
   "source": [
    "answer = rag.invoke(\"What did Lukasz contribute to the Attention Is All You Need paper?\")\n",
    "print(answer.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d672a4d5-d3a7-4d93-a314-349063d26105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (genai-conda)",
   "language": "python",
   "name": "genai-conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
