{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "885efbf6-1500-41f8-84f6-c64494ef11fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymupdf\n",
      "  Downloading pymupdf-1.26.6-cp310-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Downloading pymupdf-1.26.6-cp310-abi3-win_amd64.whl (18.4 MB)\n",
      "   ---------------------------------------- 0.0/18.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.4 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/18.4 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/18.4 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.8/18.4 MB 2.2 MB/s eta 0:00:08\n",
      "   -- ------------------------------------- 1.0/18.4 MB 1.7 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 2.1/18.4 MB 3.0 MB/s eta 0:00:06\n",
      "   ------ --------------------------------- 2.9/18.4 MB 3.2 MB/s eta 0:00:05\n",
      "   ------- -------------------------------- 3.4/18.4 MB 3.2 MB/s eta 0:00:05\n",
      "   -------- ------------------------------- 3.9/18.4 MB 3.1 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 4.5/18.4 MB 2.8 MB/s eta 0:00:05\n",
      "   --------- ------------------------------ 4.5/18.4 MB 2.8 MB/s eta 0:00:05\n",
      "   ---------- ----------------------------- 4.7/18.4 MB 2.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 4.7/18.4 MB 2.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 4.7/18.4 MB 2.4 MB/s eta 0:00:06\n",
      "   ---------- ----------------------------- 5.0/18.4 MB 1.9 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.0/18.4 MB 1.9 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.0/18.4 MB 1.9 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.0/18.4 MB 1.9 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.0/18.4 MB 1.9 MB/s eta 0:00:08\n",
      "   ---------- ----------------------------- 5.0/18.4 MB 1.9 MB/s eta 0:00:08\n",
      "   ----------- ---------------------------- 5.2/18.4 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.2/18.4 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.2/18.4 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.2/18.4 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.2/18.4 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.2/18.4 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.2/18.4 MB 1.3 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ----------- ---------------------------- 5.5/18.4 MB 984.0 kB/s eta 0:00:14\n",
      "   ------------ --------------------------- 5.8/18.4 MB 717.5 kB/s eta 0:00:18\n",
      "   ------------ --------------------------- 5.8/18.4 MB 717.5 kB/s eta 0:00:18\n",
      "   ------------ --------------------------- 5.8/18.4 MB 717.5 kB/s eta 0:00:18\n",
      "   ------------ --------------------------- 5.8/18.4 MB 717.5 kB/s eta 0:00:18\n",
      "   ------------ --------------------------- 5.8/18.4 MB 717.5 kB/s eta 0:00:18\n",
      "   ------------ --------------------------- 5.8/18.4 MB 717.5 kB/s eta 0:00:18\n",
      "   ------------ --------------------------- 5.8/18.4 MB 717.5 kB/s eta 0:00:18\n",
      "   ------------ --------------------------- 5.8/18.4 MB 717.5 kB/s eta 0:00:18\n",
      "   ------------- -------------------------- 6.0/18.4 MB 611.1 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 6.0/18.4 MB 611.1 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 6.0/18.4 MB 611.1 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 6.0/18.4 MB 611.1 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 6.0/18.4 MB 611.1 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 6.0/18.4 MB 611.1 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 6.0/18.4 MB 611.1 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 6.0/18.4 MB 611.1 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 6.0/18.4 MB 611.1 kB/s eta 0:00:21\n",
      "   ------------- -------------------------- 6.3/18.4 MB 534.5 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 6.3/18.4 MB 534.5 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 6.3/18.4 MB 534.5 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 6.3/18.4 MB 534.5 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 6.3/18.4 MB 534.5 kB/s eta 0:00:23\n",
      "   ------------- -------------------------- 6.3/18.4 MB 534.5 kB/s eta 0:00:23\n",
      "   -------------- ------------------------- 6.6/18.4 MB 497.1 kB/s eta 0:00:24\n",
      "   -------------- ------------------------- 6.6/18.4 MB 497.1 kB/s eta 0:00:24\n",
      "   -------------- ------------------------- 6.6/18.4 MB 497.1 kB/s eta 0:00:24\n",
      "   -------------- ------------------------- 6.6/18.4 MB 497.1 kB/s eta 0:00:24\n",
      "   -------------- ------------------------- 6.6/18.4 MB 497.1 kB/s eta 0:00:24\n",
      "   -------------- ------------------------- 6.6/18.4 MB 497.1 kB/s eta 0:00:24\n",
      "   -------------- ------------------------- 6.8/18.4 MB 469.7 kB/s eta 0:00:25\n",
      "   -------------- ------------------------- 6.8/18.4 MB 469.7 kB/s eta 0:00:25\n",
      "   -------------- ------------------------- 6.8/18.4 MB 469.7 kB/s eta 0:00:25\n",
      "   -------------- ------------------------- 6.8/18.4 MB 469.7 kB/s eta 0:00:25\n",
      "   -------------- ------------------------- 6.8/18.4 MB 469.7 kB/s eta 0:00:25\n",
      "   -------------- ------------------------- 6.8/18.4 MB 469.7 kB/s eta 0:00:25\n",
      "   -------------- ------------------------- 6.8/18.4 MB 469.7 kB/s eta 0:00:25\n",
      "   -------------- ------------------------- 6.8/18.4 MB 469.7 kB/s eta 0:00:25\n",
      "   -------------- ------------------------- 6.8/18.4 MB 469.7 kB/s eta 0:00:25\n",
      "   --------------- ------------------------ 7.1/18.4 MB 431.9 kB/s eta 0:00:27\n",
      "   --------------- ------------------------ 7.1/18.4 MB 431.9 kB/s eta 0:00:27\n",
      "   --------------- ------------------------ 7.1/18.4 MB 431.9 kB/s eta 0:00:27\n",
      "   --------------- ------------------------ 7.1/18.4 MB 431.9 kB/s eta 0:00:27\n",
      "   --------------- ------------------------ 7.1/18.4 MB 431.9 kB/s eta 0:00:27\n",
      "   --------------- ------------------------ 7.1/18.4 MB 431.9 kB/s eta 0:00:27\n",
      "   --------------- ------------------------ 7.3/18.4 MB 416.7 kB/s eta 0:00:27\n",
      "   --------------- ------------------------ 7.3/18.4 MB 416.7 kB/s eta 0:00:27\n",
      "   --------------- ------------------------ 7.3/18.4 MB 416.7 kB/s eta 0:00:27\n",
      "   --------------- ------------------------ 7.3/18.4 MB 416.7 kB/s eta 0:00:27\n",
      "   --------------- ------------------------ 7.3/18.4 MB 416.7 kB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 7.6/18.4 MB 404.3 kB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 7.6/18.4 MB 404.3 kB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 7.6/18.4 MB 404.3 kB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 7.6/18.4 MB 404.3 kB/s eta 0:00:27\n",
      "   ---------------- ----------------------- 7.6/18.4 MB 404.3 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 7.9/18.4 MB 395.9 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 7.9/18.4 MB 395.9 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 7.9/18.4 MB 395.9 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 7.9/18.4 MB 395.9 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 7.9/18.4 MB 395.9 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 7.9/18.4 MB 395.9 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 7.9/18.4 MB 395.9 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 7.9/18.4 MB 395.9 kB/s eta 0:00:27\n",
      "   ----------------- ---------------------- 8.1/18.4 MB 378.1 kB/s eta 0:00:28\n",
      "   ----------------- ---------------------- 8.1/18.4 MB 378.1 kB/s eta 0:00:28\n",
      "   ----------------- ---------------------- 8.1/18.4 MB 378.1 kB/s eta 0:00:28\n",
      "   ----------------- ---------------------- 8.1/18.4 MB 378.1 kB/s eta 0:00:28\n",
      "   ----------------- ---------------------- 8.1/18.4 MB 378.1 kB/s eta 0:00:28\n",
      "   ----------------- ---------------------- 8.1/18.4 MB 378.1 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.4/18.4 MB 366.5 kB/s eta 0:00:28\n",
      "   ------------------ --------------------- 8.7/18.4 MB 317.3 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 8.7/18.4 MB 317.3 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 8.7/18.4 MB 317.3 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 8.7/18.4 MB 317.3 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 8.7/18.4 MB 317.3 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 8.7/18.4 MB 317.3 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 8.7/18.4 MB 317.3 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 8.7/18.4 MB 317.3 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 8.7/18.4 MB 317.3 kB/s eta 0:00:31\n",
      "   ------------------ --------------------- 8.7/18.4 MB 317.3 kB/s eta 0:00:31\n",
      "   ------------------- -------------------- 8.9/18.4 MB 301.4 kB/s eta 0:00:32\n",
      "   ------------------- -------------------- 8.9/18.4 MB 301.4 kB/s eta 0:00:32\n",
      "   ------------------- -------------------- 8.9/18.4 MB 301.4 kB/s eta 0:00:32\n",
      "   ------------------- -------------------- 8.9/18.4 MB 301.4 kB/s eta 0:00:32\n",
      "   ------------------- -------------------- 8.9/18.4 MB 301.4 kB/s eta 0:00:32\n",
      "   ------------------- -------------------- 8.9/18.4 MB 301.4 kB/s eta 0:00:32\n",
      "   ------------------- -------------------- 8.9/18.4 MB 301.4 kB/s eta 0:00:32\n",
      "   ------------------- -------------------- 8.9/18.4 MB 301.4 kB/s eta 0:00:32\n",
      "   ------------------- -------------------- 8.9/18.4 MB 301.4 kB/s eta 0:00:32\n",
      "   ------------------- -------------------- 9.2/18.4 MB 236.2 kB/s eta 0:00:40\n",
      "   ------------------- -------------------- 9.2/18.4 MB 236.2 kB/s eta 0:00:40\n",
      "   ------------------- -------------------- 9.2/18.4 MB 236.2 kB/s eta 0:00:40\n",
      "   ------------------- -------------------- 9.2/18.4 MB 236.2 kB/s eta 0:00:40\n",
      "   ------------------- -------------------- 9.2/18.4 MB 236.2 kB/s eta 0:00:40\n",
      "   ------------------- -------------------- 9.2/18.4 MB 236.2 kB/s eta 0:00:40\n",
      "   ------------------- -------------------- 9.2/18.4 MB 236.2 kB/s eta 0:00:40\n",
      "   -------------------- ------------------- 9.4/18.4 MB 151.1 kB/s eta 0:01:00\n",
      "   -------------------- ------------------- 9.4/18.4 MB 151.1 kB/s eta 0:01:00\n",
      "   -------------------- ------------------- 9.4/18.4 MB 151.1 kB/s eta 0:01:00\n",
      "   -------------------- ------------------- 9.4/18.4 MB 151.1 kB/s eta 0:01:00\n",
      "   -------------------- ------------------- 9.4/18.4 MB 151.1 kB/s eta 0:01:00\n",
      "   --------------------- ------------------ 9.7/18.4 MB 151.7 kB/s eta 0:00:58\n",
      "   --------------------- ------------------ 9.7/18.4 MB 151.7 kB/s eta 0:00:58\n",
      "   --------------------- ------------------ 9.7/18.4 MB 151.7 kB/s eta 0:00:58\n",
      "   --------------------- ------------------ 9.7/18.4 MB 151.7 kB/s eta 0:00:58\n",
      "   --------------------- ------------------ 9.7/18.4 MB 151.7 kB/s eta 0:00:58\n",
      "   --------------------- ------------------ 10.0/18.4 MB 154.6 kB/s eta 0:00:55\n",
      "   --------------------- ------------------ 10.0/18.4 MB 154.6 kB/s eta 0:00:55\n",
      "   --------------------- ------------------ 10.0/18.4 MB 154.6 kB/s eta 0:00:55\n",
      "   --------------------- ------------------ 10.0/18.4 MB 154.6 kB/s eta 0:00:55\n",
      "   --------------------- ------------------ 10.0/18.4 MB 154.6 kB/s eta 0:00:55\n",
      "   ---------------------- ----------------- 10.2/18.4 MB 158.3 kB/s eta 0:00:52\n",
      "   ---------------------- ----------------- 10.2/18.4 MB 158.3 kB/s eta 0:00:52\n",
      "   ---------------------- ----------------- 10.2/18.4 MB 158.3 kB/s eta 0:00:52\n",
      "   ---------------------- ----------------- 10.2/18.4 MB 158.3 kB/s eta 0:00:52\n",
      "   ---------------------- ----------------- 10.2/18.4 MB 158.3 kB/s eta 0:00:52\n",
      "   ---------------------- ----------------- 10.5/18.4 MB 165.8 kB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 10.5/18.4 MB 165.8 kB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 10.5/18.4 MB 165.8 kB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 10.5/18.4 MB 165.8 kB/s eta 0:00:48\n",
      "   ---------------------- ----------------- 10.5/18.4 MB 165.8 kB/s eta 0:00:48\n",
      "   ----------------------- ---------------- 10.7/18.4 MB 167.7 kB/s eta 0:00:46\n",
      "   ----------------------- ---------------- 10.7/18.4 MB 167.7 kB/s eta 0:00:46\n",
      "   ----------------------- ---------------- 10.7/18.4 MB 167.7 kB/s eta 0:00:46\n",
      "   ----------------------- ---------------- 10.7/18.4 MB 167.7 kB/s eta 0:00:46\n",
      "   ----------------------- ---------------- 10.7/18.4 MB 167.7 kB/s eta 0:00:46\n",
      "   ----------------------- ---------------- 11.0/18.4 MB 172.4 kB/s eta 0:00:43\n",
      "   ----------------------- ---------------- 11.0/18.4 MB 172.4 kB/s eta 0:00:43\n",
      "   ----------------------- ---------------- 11.0/18.4 MB 172.4 kB/s eta 0:00:43\n",
      "   ----------------------- ---------------- 11.0/18.4 MB 172.4 kB/s eta 0:00:43\n",
      "   ----------------------- ---------------- 11.0/18.4 MB 172.4 kB/s eta 0:00:43\n",
      "   ----------------------- ---------------- 11.0/18.4 MB 172.4 kB/s eta 0:00:43\n",
      "   ----------------------- ---------------- 11.0/18.4 MB 172.4 kB/s eta 0:00:43\n",
      "   ----------------------- ---------------- 11.0/18.4 MB 172.4 kB/s eta 0:00:43\n",
      "   ------------------------ --------------- 11.3/18.4 MB 174.0 kB/s eta 0:00:41\n",
      "   ------------------------ --------------- 11.3/18.4 MB 174.0 kB/s eta 0:00:41\n",
      "   ------------------------ --------------- 11.3/18.4 MB 174.0 kB/s eta 0:00:41\n",
      "   ------------------------ --------------- 11.3/18.4 MB 174.0 kB/s eta 0:00:41\n",
      "   ------------------------ --------------- 11.3/18.4 MB 174.0 kB/s eta 0:00:41\n",
      "   ------------------------ --------------- 11.3/18.4 MB 174.0 kB/s eta 0:00:41\n",
      "   ------------------------- -------------- 11.5/18.4 MB 175.0 kB/s eta 0:00:40\n",
      "   ------------------------- -------------- 11.5/18.4 MB 175.0 kB/s eta 0:00:40\n",
      "   ------------------------- -------------- 11.5/18.4 MB 175.0 kB/s eta 0:00:40\n",
      "   ------------------------- -------------- 11.5/18.4 MB 175.0 kB/s eta 0:00:40\n",
      "   ------------------------- -------------- 11.8/18.4 MB 177.2 kB/s eta 0:00:38\n",
      "   ------------------------- -------------- 11.8/18.4 MB 177.2 kB/s eta 0:00:38\n",
      "   ------------------------- -------------- 11.8/18.4 MB 177.2 kB/s eta 0:00:38\n",
      "   ------------------------- -------------- 11.8/18.4 MB 177.2 kB/s eta 0:00:38\n",
      "   ------------------------- -------------- 11.8/18.4 MB 177.2 kB/s eta 0:00:38\n",
      "   -------------------------- ------------- 12.1/18.4 MB 179.0 kB/s eta 0:00:36\n",
      "   -------------------------- ------------- 12.1/18.4 MB 179.0 kB/s eta 0:00:36\n",
      "   -------------------------- ------------- 12.1/18.4 MB 179.0 kB/s eta 0:00:36\n",
      "   -------------------------- ------------- 12.1/18.4 MB 179.0 kB/s eta 0:00:36\n",
      "   -------------------------- ------------- 12.1/18.4 MB 179.0 kB/s eta 0:00:36\n",
      "   -------------------------- ------------- 12.1/18.4 MB 179.0 kB/s eta 0:00:36\n",
      "   -------------------------- ------------- 12.1/18.4 MB 179.0 kB/s eta 0:00:36\n",
      "   -------------------------- ------------- 12.3/18.4 MB 181.4 kB/s eta 0:00:34\n",
      "   -------------------------- ------------- 12.3/18.4 MB 181.4 kB/s eta 0:00:34\n",
      "   -------------------------- ------------- 12.3/18.4 MB 181.4 kB/s eta 0:00:34\n",
      "   -------------------------- ------------- 12.3/18.4 MB 181.4 kB/s eta 0:00:34\n",
      "   -------------------------- ------------- 12.3/18.4 MB 181.4 kB/s eta 0:00:34\n",
      "   -------------------------- ------------- 12.3/18.4 MB 181.4 kB/s eta 0:00:34\n",
      "   -------------------------- ------------- 12.3/18.4 MB 181.4 kB/s eta 0:00:34\n",
      "   --------------------------- ------------ 12.6/18.4 MB 180.1 kB/s eta 0:00:33\n",
      "   --------------------------- ------------ 12.6/18.4 MB 180.1 kB/s eta 0:00:33\n",
      "   --------------------------- ------------ 12.6/18.4 MB 180.1 kB/s eta 0:00:33\n",
      "   --------------------------- ------------ 12.6/18.4 MB 180.1 kB/s eta 0:00:33\n",
      "   --------------------------- ------------ 12.6/18.4 MB 180.1 kB/s eta 0:00:33\n",
      "   --------------------------- ------------ 12.6/18.4 MB 180.1 kB/s eta 0:00:33\n",
      "   --------------------------- ------------ 12.8/18.4 MB 179.0 kB/s eta 0:00:32\n",
      "   --------------------------- ------------ 12.8/18.4 MB 179.0 kB/s eta 0:00:32\n",
      "   --------------------------- ------------ 12.8/18.4 MB 179.0 kB/s eta 0:00:32\n",
      "   --------------------------- ------------ 12.8/18.4 MB 179.0 kB/s eta 0:00:32\n",
      "   --------------------------- ------------ 12.8/18.4 MB 179.0 kB/s eta 0:00:32\n",
      "   --------------------------- ------------ 12.8/18.4 MB 179.0 kB/s eta 0:00:32\n",
      "   --------------------------- ------------ 12.8/18.4 MB 179.0 kB/s eta 0:00:32\n",
      "   ---------------------------- ----------- 13.1/18.4 MB 176.6 kB/s eta 0:00:31\n",
      "   ---------------------------- ----------- 13.1/18.4 MB 176.6 kB/s eta 0:00:31\n",
      "   ---------------------------- ----------- 13.1/18.4 MB 176.6 kB/s eta 0:00:31\n",
      "   ---------------------------- ----------- 13.1/18.4 MB 176.6 kB/s eta 0:00:31\n",
      "   ---------------------------- ----------- 13.1/18.4 MB 176.6 kB/s eta 0:00:31\n",
      "   ---------------------------- ----------- 13.1/18.4 MB 176.6 kB/s eta 0:00:31\n",
      "   ----------------------------- ---------- 13.4/18.4 MB 179.1 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 13.4/18.4 MB 179.1 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 13.4/18.4 MB 179.1 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 13.4/18.4 MB 179.1 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 13.4/18.4 MB 179.1 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 13.4/18.4 MB 179.1 kB/s eta 0:00:29\n",
      "   ----------------------------- ---------- 13.6/18.4 MB 179.9 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 13.6/18.4 MB 179.9 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 13.6/18.4 MB 179.9 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 13.6/18.4 MB 179.9 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 13.6/18.4 MB 179.9 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 13.6/18.4 MB 179.9 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 13.6/18.4 MB 179.9 kB/s eta 0:00:27\n",
      "   ----------------------------- ---------- 13.6/18.4 MB 179.9 kB/s eta 0:00:27\n",
      "   ------------------------------ --------- 13.9/18.4 MB 197.3 kB/s eta 0:00:23\n",
      "   ------------------------------ --------- 13.9/18.4 MB 197.3 kB/s eta 0:00:23\n",
      "   ------------------------------ --------- 13.9/18.4 MB 197.3 kB/s eta 0:00:23\n",
      "   ------------------------------ --------- 13.9/18.4 MB 197.3 kB/s eta 0:00:23\n",
      "   ------------------------------ --------- 13.9/18.4 MB 197.3 kB/s eta 0:00:23\n",
      "   ------------------------------ --------- 13.9/18.4 MB 197.3 kB/s eta 0:00:23\n",
      "   ------------------------------ --------- 13.9/18.4 MB 197.3 kB/s eta 0:00:23\n",
      "   ------------------------------ --------- 14.2/18.4 MB 195.3 kB/s eta 0:00:22\n",
      "   ------------------------------ --------- 14.2/18.4 MB 195.3 kB/s eta 0:00:22\n",
      "   ------------------------------ --------- 14.2/18.4 MB 195.3 kB/s eta 0:00:22\n",
      "   ------------------------------ --------- 14.2/18.4 MB 195.3 kB/s eta 0:00:22\n",
      "   ------------------------------ --------- 14.2/18.4 MB 195.3 kB/s eta 0:00:22\n",
      "   ------------------------------ --------- 14.2/18.4 MB 195.3 kB/s eta 0:00:22\n",
      "   ------------------------------ --------- 14.2/18.4 MB 195.3 kB/s eta 0:00:22\n",
      "   ------------------------------ --------- 14.2/18.4 MB 195.3 kB/s eta 0:00:22\n",
      "   ------------------------------ --------- 14.2/18.4 MB 195.3 kB/s eta 0:00:22\n",
      "   ------------------------------- -------- 14.4/18.4 MB 198.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 14.4/18.4 MB 198.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 14.4/18.4 MB 198.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 14.4/18.4 MB 198.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 14.4/18.4 MB 198.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 14.4/18.4 MB 198.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 14.4/18.4 MB 198.3 kB/s eta 0:00:21\n",
      "   ------------------------------- -------- 14.7/18.4 MB 196.7 kB/s eta 0:00:19\n",
      "   ------------------------------- -------- 14.7/18.4 MB 196.7 kB/s eta 0:00:19\n",
      "   ------------------------------- -------- 14.7/18.4 MB 196.7 kB/s eta 0:00:19\n",
      "   ------------------------------- -------- 14.7/18.4 MB 196.7 kB/s eta 0:00:19\n",
      "   ------------------------------- -------- 14.7/18.4 MB 196.7 kB/s eta 0:00:19\n",
      "   ------------------------------- -------- 14.7/18.4 MB 196.7 kB/s eta 0:00:19\n",
      "   ------------------------------- -------- 14.7/18.4 MB 196.7 kB/s eta 0:00:19\n",
      "   ------------------------------- -------- 14.7/18.4 MB 196.7 kB/s eta 0:00:19\n",
      "   -------------------------------- ------- 14.9/18.4 MB 198.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 14.9/18.4 MB 198.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 14.9/18.4 MB 198.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 14.9/18.4 MB 198.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 14.9/18.4 MB 198.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 14.9/18.4 MB 198.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 14.9/18.4 MB 198.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 14.9/18.4 MB 198.3 kB/s eta 0:00:18\n",
      "   -------------------------------- ------- 14.9/18.4 MB 198.3 kB/s eta 0:00:18\n",
      "   --------------------------------- ------ 15.2/18.4 MB 194.7 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 15.2/18.4 MB 194.7 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 15.2/18.4 MB 194.7 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 15.2/18.4 MB 194.7 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 15.2/18.4 MB 194.7 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 15.2/18.4 MB 194.7 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 15.2/18.4 MB 194.7 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 15.2/18.4 MB 194.7 kB/s eta 0:00:17\n",
      "   --------------------------------- ------ 15.5/18.4 MB 189.0 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 15.5/18.4 MB 189.0 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 15.5/18.4 MB 189.0 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 15.5/18.4 MB 189.0 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 15.5/18.4 MB 189.0 kB/s eta 0:00:16\n",
      "   --------------------------------- ------ 15.5/18.4 MB 189.0 kB/s eta 0:00:16\n",
      "   ---------------------------------- ----- 15.7/18.4 MB 187.2 kB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 15.7/18.4 MB 187.2 kB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 15.7/18.4 MB 187.2 kB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 15.7/18.4 MB 187.2 kB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 15.7/18.4 MB 187.2 kB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 15.7/18.4 MB 187.2 kB/s eta 0:00:15\n",
      "   ---------------------------------- ----- 16.0/18.4 MB 186.2 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 16.0/18.4 MB 186.2 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 16.0/18.4 MB 186.2 kB/s eta 0:00:13\n",
      "   ---------------------------------- ----- 16.0/18.4 MB 186.2 kB/s eta 0:00:13\n",
      "   ----------------------------------- ---- 16.3/18.4 MB 187.9 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 16.3/18.4 MB 187.9 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 16.3/18.4 MB 187.9 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 16.3/18.4 MB 187.9 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 16.3/18.4 MB 187.9 kB/s eta 0:00:12\n",
      "   ----------------------------------- ---- 16.5/18.4 MB 187.8 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 16.5/18.4 MB 187.8 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 16.5/18.4 MB 187.8 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 16.5/18.4 MB 187.8 kB/s eta 0:00:11\n",
      "   ----------------------------------- ---- 16.5/18.4 MB 187.8 kB/s eta 0:00:11\n",
      "   ------------------------------------ --- 16.8/18.4 MB 191.5 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 16.8/18.4 MB 191.5 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 16.8/18.4 MB 191.5 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 16.8/18.4 MB 191.5 kB/s eta 0:00:09\n",
      "   ------------------------------------ --- 16.8/18.4 MB 191.5 kB/s eta 0:00:09\n",
      "   ------------------------------------- -- 17.0/18.4 MB 194.0 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 17.0/18.4 MB 194.0 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 17.0/18.4 MB 194.0 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 17.0/18.4 MB 194.0 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 17.0/18.4 MB 194.0 kB/s eta 0:00:08\n",
      "   ------------------------------------- -- 17.3/18.4 MB 195.7 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 17.3/18.4 MB 195.7 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 17.3/18.4 MB 195.7 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 17.3/18.4 MB 195.7 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 17.3/18.4 MB 195.7 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 17.3/18.4 MB 195.7 kB/s eta 0:00:06\n",
      "   ------------------------------------- -- 17.3/18.4 MB 195.7 kB/s eta 0:00:06\n",
      "   -------------------------------------- - 17.6/18.4 MB 189.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 17.6/18.4 MB 189.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 17.6/18.4 MB 189.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 17.6/18.4 MB 189.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 17.6/18.4 MB 189.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 17.6/18.4 MB 189.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 17.6/18.4 MB 189.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 17.6/18.4 MB 189.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 17.6/18.4 MB 189.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 17.6/18.4 MB 189.5 kB/s eta 0:00:05\n",
      "   -------------------------------------- - 17.8/18.4 MB 185.7 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 17.8/18.4 MB 185.7 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 17.8/18.4 MB 185.7 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 17.8/18.4 MB 185.7 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 17.8/18.4 MB 185.7 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 17.8/18.4 MB 185.7 kB/s eta 0:00:04\n",
      "   -------------------------------------- - 17.8/18.4 MB 185.7 kB/s eta 0:00:04\n",
      "   ---------------------------------------  18.1/18.4 MB 184.9 kB/s eta 0:00:02\n",
      "   ---------------------------------------  18.1/18.4 MB 184.9 kB/s eta 0:00:02\n",
      "   ---------------------------------------  18.1/18.4 MB 184.9 kB/s eta 0:00:02\n",
      "   ---------------------------------------  18.1/18.4 MB 184.9 kB/s eta 0:00:02\n",
      "   ---------------------------------------  18.1/18.4 MB 184.9 kB/s eta 0:00:02\n",
      "   ---------------------------------------  18.4/18.4 MB 186.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------  18.4/18.4 MB 186.6 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 18.4/18.4 MB 186.2 kB/s  0:01:17\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.6\n",
      "Collecting pymupdf\n",
      "  Using cached pymupdf-1.26.6-cp310-abi3-win_amd64.whl.metadata (3.4 kB)\n",
      "Using cached pymupdf-1.26.6-cp310-abi3-win_amd64.whl (18.4 MB)\n",
      "Installing collected packages: pymupdf\n",
      "Successfully installed pymupdf-1.26.6\n"
     ]
    }
   ],
   "source": [
    "!pip install pymupdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392aeb65-abab-4fc2-a87e-200ba9ed331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==== RAG Pipeline for LangChain v1.x (LCEL) ====\n",
    "# langchain==1.0.x / core==1.0.x / community==0.4.x\n",
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict\n",
    "\n",
    "# community integrations\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from typing import List, Tuple\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS, DistanceStrategy\n",
    "from langchain_community.llms import HuggingFaceHub  # swap if you prefer another LLM\n",
    "\n",
    "import math\n",
    "# LCEL primitives\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3aaf0ea-e9e5-4bd5-a02b-3e25a14109e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HUGGINGFACEHUB_API_TOKEN set? True\n",
      "GEMINI_API_KEY set? True\n",
      "‚ö†Ô∏è Gemini key appears default or missing ‚Äî skipping configuration\n"
     ]
    }
   ],
   "source": [
    "# --- set your keys (use environment vars for safety) ---\n",
    "# export or set them here before running:\n",
    "# os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = \"hf_xxx\"\n",
    "# os.environ[\"GEMINI_API_KEY\"] = \"your_gemini_key\"\n",
    "\n",
    "HF_TOKEN = \"\"\n",
    "GEMINI_KEY = \"\"\n",
    "\n",
    "\n",
    "# 1Ô∏è‚É£ Set environment variables so all later cells can use them\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n",
    "os.environ[\"GEMINI_API_KEY\"] = GEMINI_KEY\n",
    "\n",
    "\n",
    "# 2Ô∏è‚É£ Basic validation prints\n",
    "print(\"HUGGINGFACEHUB_API_TOKEN set?\", bool(os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")))\n",
    "print(\"GEMINI_API_KEY set?\", bool(os.getenv(\"GEMINI_API_KEY\")))\n",
    "\n",
    "# 3Ô∏è‚É£ Configure Gemini SDK if key provided\n",
    "if GEMINI_KEY and not GEMINI_KEY.startswith(\"AIzaSyAl\"):\n",
    "    genai.configure(api_key=GEMINI_KEY)\n",
    "    print(\"‚úÖ Gemini API configured\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Gemini key appears default or missing ‚Äî skipping configuration\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fe8f55b-a07f-44b7-a5c9-c0b919e977fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 15 documents (pages). Example page content head:\n",
      " Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani‚àó\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer‚àó\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar‚àó\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit‚àó\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones‚àó\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez‚àó‚Ä†\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "≈Åukasz Kaiser‚àó\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin‚àó‚Ä°\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and \n"
     ]
    }
   ],
   "source": [
    "# interactive upload, will be useful in streamlit\n",
    "\n",
    "\n",
    "# from IPython.display import display\n",
    "# import ipywidgets as widgets\n",
    "# import shutil\n",
    "\n",
    "# upload = widgets.FileUpload(accept='.pdf', multiple=False)\n",
    "# display(upload)\n",
    "\n",
    "# # after uploading\n",
    "# def save_uploaded_pdf(upload_widget):\n",
    "#     for filename, file_info in upload_widget.value.items():\n",
    "#         with open(filename, 'wb') as f:\n",
    "#             f.write(file_info['content'])\n",
    "#         return filename\n",
    "\n",
    "# pdf_path = save_uploaded_pdf(upload)\n",
    "# print(\"Saved PDF:\", pdf_path)\n",
    "\n",
    "# Replace 'sample.pdf' with your PDF path (you can iterate a list of files)\n",
    "from langchain_community.document_loaders import PyMuPDFLoader\n",
    "pdf_path = \"./sample.pdf\"  # Replace with your PDF path\n",
    "loader = PyMuPDFLoader(pdf_path)\n",
    "docs = loader.load()  # Returns list of Document objects (each page typically)\n",
    "\n",
    "print(f\"Loaded {len(docs)} documents (pages). Example page content head:\\n\", docs[0].page_content[:800])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "443081d8-29c2-4fe1-a965-1f6e20541d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Deeply cleaned doc 1 (first 800 chars) ---\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@google.com Noam Shazeer‚àó Google Brain noam@google.com Niki Parmar‚àó Google Research nikip@google.com Jakob Uszkoreit‚àó Google Research usz@google.com Llion Jones‚àó Google Research llion@google.com Aidan N. Gomez‚àó‚Ä† University of Toronto aidan@cs.toronto.edu ≈Åukasz Kaiser‚àó Google Brain lukaszkaiser@google.com Illia Polosukhin‚àó‚Ä° illia.polosukhin@gmail.com Abstract The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and \n",
      "\n",
      "--- End preview ---\n",
      "\n",
      "\n",
      "--- Deeply cleaned doc 2 (first 800 chars) ---\n",
      "\n",
      "1 Introduction Recurrent neural networks, long short-term memory and gated recurrent neural networks in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [3, 2, 5]. Numerous efforts have since continued to push the boundaries of recurrent language models and encoder-decoder architectures [3, 2, 15]. Recurrent models typically factor computation along the symbol positions of the input and output sequences. Aligning the positions to steps in computation time, they generate a sequence of hidden states ht, as a function of the previous hidden state ht‚àí1 and the input for position t. This inherently sequential nature precludes parallelization within training examples, which bec\n",
      "\n",
      "--- End preview ---\n",
      "\n",
      "‚úÖ Cleaned pages: 15\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cell ‚Äî Deep clean PDF pages with enriched metadata\n",
    "# ============================\n",
    "import re\n",
    "import unicodedata\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "def improved_clean(documents, file_name):\n",
    "    cleaned_docs = []\n",
    "    for doc in documents:\n",
    "        text = doc.page_content\n",
    "        page_number = doc.metadata.get(\"page\")  # Retrieve page number from metadata\n",
    "        \n",
    "        # Dummy metadata - replace with actual extraction logic (can be improved)\n",
    "        title = \"Sample Research Paper\"  # This should be extracted from the PDF title or document header\n",
    "        authors = \"John Doe, Jane Smith\"  # Extract authors from metadata or first pages\n",
    "        publication_date = \"2024-01-01\"  # Extract publication date if available\n",
    "        source_link = \"https://arxiv.org/abs/123456\"  # If available in the document metadata\n",
    "\n",
    "        # 1) Unicode normalize (fix ligatures / odd widths)\n",
    "        text = unicodedata.normalize(\"NFKC\", text)\n",
    "\n",
    "        # 2) Repair hyphenation across line breaks\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1-\\2', text)\n",
    "\n",
    "        # 3) Preserve paragraph breaks\n",
    "        text = re.sub(r'\\n{2,}', '<PAR>', text)  # mark paragraphs\n",
    "        text = re.sub(r'[\\r\\n]+', ' ', text)     # flatten single newlines\n",
    "\n",
    "        # 4) Remove bracketed numeric citations like [12]\n",
    "        text = re.sub(r'\\[\\s*\\d+\\s*\\]', '', text)\n",
    "\n",
    "        # 5) Remove inline trailing citation digits glued to words (e.g., intelligence1.)\n",
    "        text = re.sub(r'(?<=\\w)(\\d{1,3})(?=[\\s\\.,;:])', '', text)\n",
    "\n",
    "        # 6) Remove long repeated digit runs (e.g., 1111, 1515151)\n",
    "        text = re.sub(r'(\\d)\\1{3,}', '', text)\n",
    "\n",
    "        # 7) Remove \"Page 12\" style markers\n",
    "        text = re.sub(r'\\bPage\\s*\\d+\\b', '', text, flags=re.IGNORECASE)\n",
    "\n",
    "        # 8) Strip control chars\n",
    "        text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C')\n",
    "\n",
    "        # 9) Normalize whitespace and restore paragraph breaks\n",
    "        text = re.sub(r'[ \\t\\f\\v]+', ' ', text)  # collapse horizontal whitespace\n",
    "        text = text.replace('<PAR>', '\\n\\n')     # restore paragraphs\n",
    "        text = re.sub(r' {2,}', ' ', text).strip()\n",
    "\n",
    "        # 10) Targeted glyph fixes (extend if you see more)\n",
    "        replacements = {\n",
    "            'Trade-o∆Ø': 'Trade-off',\n",
    "            'Tradeo∆Ø': 'Trade-off',\n",
    "            'oe∆Ø': 'oeff',\n",
    "            'coe∆Ø': 'coeff',\n",
    "            '∆Ø': 'f',  # keep last: broadest\n",
    "        }\n",
    "        for k, v in replacements.items():\n",
    "            text = text.replace(k, v)\n",
    "\n",
    "        # Assign metadata (file_name, page number, title, authors, date, source_link)\n",
    "        doc.metadata[\"source\"] = file_name\n",
    "        doc.metadata[\"page\"] = page_number\n",
    "        doc.metadata[\"title\"] = title\n",
    "        doc.metadata[\"authors\"] = authors\n",
    "        doc.metadata[\"publication_date\"] = publication_date\n",
    "        doc.metadata[\"source_link\"] = source_link\n",
    "\n",
    "        # Determine section heading\n",
    "        if page_number in [\"0\", \"1\"]:\n",
    "            doc.metadata[\"section_heading\"] = \"authors\"\n",
    "        elif \"references\" in text.lower() or \"bibliography\" in text.lower():\n",
    "            doc.metadata[\"section_heading\"] = \"references\"\n",
    "        else:\n",
    "            doc.metadata[\"section_heading\"] = \"body\"\n",
    "\n",
    "        # Append the cleaned document with updated metadata\n",
    "        cleaned_docs.append(Document(page_content=text, metadata=doc.metadata))\n",
    "\n",
    "    return cleaned_docs\n",
    "\n",
    "# Apply to 'docs' (output of PyMuPDFLoader.load())\n",
    "docs_deeper_cleaned = improved_clean(docs, \"sample.pdf\")  # Pass the file name for metadata\n",
    "\n",
    "# Safe previews for first 2 cleaned docs\n",
    "for i, d in enumerate(docs_deeper_cleaned[:2]):\n",
    "    print(f\"\\n--- Deeply cleaned doc {i+1} (first 800 chars) ---\\n\")\n",
    "    print(d.page_content[:800])\n",
    "    print(\"\\n--- End preview ---\\n\")\n",
    "\n",
    "print(f\"‚úÖ Cleaned pages: {len(docs_deeper_cleaned)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c8bef20b-6c1c-4aa7-b456-5ad9d7c5a918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Total chunks created: 95\n",
      "üìÑ Example chunk preview:\n",
      "\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@google.com Noam Shazeer‚àó Google Brain noam@google.com Niki Parmar‚àó Google Research nikip@google.com Jakob Uszkoreit‚àó Google Research usz@google.com Llion Jones‚àó Google Research llion@google.com Aidan N\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Initialize your text splitter (tune separators if your PDF lost newlines)\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=30,\n",
    "    add_start_index=True,  # Keep character offsets for traceability\n",
    "    separators=(\"\\\\n\\\\n\", \"\\\\n\", \". \", \" \", \"\")  # Optional: control where splits happen\n",
    ")\n",
    "\n",
    "# Preferred: Split documents directly (preserves metadata)\n",
    "chunked_docs = splitter.split_documents(docs_deeper_cleaned)\n",
    "\n",
    "# Add per-chunk metadata for traceability\n",
    "for i, doc in enumerate(chunked_docs):\n",
    "    # Retrieve start index from original document's metadata\n",
    "    start = doc.metadata.get(\"start_index\")\n",
    "    if start is not None:\n",
    "        doc.metadata[\"char_start\"] = start\n",
    "        doc.metadata[\"char_end\"] = start + len(doc.page_content)\n",
    "        del doc.metadata[\"start_index\"]\n",
    "\n",
    "    # Add chunk-specific metadata\n",
    "    doc.metadata[\"chunk_id\"] = i\n",
    "    # Copy other relevant metadata from the original document\n",
    "    doc.metadata[\"title\"] = docs_deeper_cleaned[i % len(docs_deeper_cleaned)].metadata.get(\"title\", \"Unknown Title\")\n",
    "    doc.metadata[\"authors\"] = docs_deeper_cleaned[i % len(docs_deeper_cleaned)].metadata.get(\"authors\", \"Unknown Authors\")\n",
    "    doc.metadata[\"publication_date\"] = docs_deeper_cleaned[i % len(docs_deeper_cleaned)].metadata.get(\"publication_date\", \"Unknown Date\")\n",
    "    doc.metadata[\"section_heading\"] = docs_deeper_cleaned[i % len(docs_deeper_cleaned)].metadata.get(\"section_heading\", \"Unknown Section\")\n",
    "\n",
    "print(f\"‚úÖ Total chunks created: {len(chunked_docs)}\")\n",
    "if chunked_docs:\n",
    "    print(\"üìÑ Example chunk preview:\\n\")\n",
    "    print(chunked_docs[0].page_content[:500])\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No chunks produced. Check upstream cleaning or splitter settings.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f49c474f-5c44-4bd5-8c53-80bfc92a1270",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 95 chunks to chunks.json\n",
      "üìÑ Example chunk preview:\n",
      "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@google.com Noam Shazeer‚àó Google Brain noam@google.com Niki Parmar‚àó Google Research nikip@google.com Jakob Uszkoreit‚àó Google Research usz@google.com Llion Jones‚àó G\n"
     ]
    }
   ],
   "source": [
    "import json  # ‚úÖ add this import\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# --- STEP 1: Split documents ---\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=512,\n",
    "    chunk_overlap=30,\n",
    "    add_start_index=True,\n",
    "    separators=(\"\\n\\n\", \"\\n\", \". \", \" \", \"\"),   # safer: use actual newlines, not escaped\n",
    ")\n",
    "\n",
    "chunked_docs = splitter.split_documents(docs_deeper_cleaned)\n",
    "\n",
    "# --- STEP 2: Add metadata per chunk ---\n",
    "for i, doc in enumerate(chunked_docs):\n",
    "    start = doc.metadata.get(\"start_index\")\n",
    "    if start is not None:\n",
    "        doc.metadata[\"char_start\"] = start\n",
    "        doc.metadata[\"char_end\"] = start + len(doc.page_content)\n",
    "        del doc.metadata[\"start_index\"]\n",
    "    doc.metadata[\"chunk_id\"] = i\n",
    "\n",
    "# --- STEP 3: Store chunks in JSON file ---\n",
    "chunks_data = [\n",
    "    {\n",
    "        \"text\": doc.page_content,\n",
    "        \"metadata\": doc.metadata\n",
    "    }\n",
    "    for doc in chunked_docs\n",
    "]\n",
    "\n",
    "output_path = \"chunks.json\"\n",
    "with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunks_data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"‚úÖ Saved {len(chunks_data)} chunks to {output_path}\")\n",
    "print(\"üìÑ Example chunk preview:\")\n",
    "print(chunks_data[0][\"text\"][:400])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e2a8521-dfc8-4993-846e-2c3cff7d0c42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 95 chunks from chunks.json\n",
      "Min norm: 1.0, Max norm: 1.0, Mean norm: 1.0\n",
      "FAISS Index Type: <class 'faiss.swigfaiss_avx2.IndexFlatIP'>\n",
      "FAISS Metric Type: 0\n",
      "‚úÖ Total embeddings created: 10\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ FAISS setup with Safe Embeddings and Cosine Distance (LangChain v1)\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from langchain_core.documents import Document\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS, DistanceStrategy\n",
    "# from langchain_huggingface import HuggingFaceEmbeddings\n",
    "# from langchain_community.vectorstores import FAISS, DistanceStrategy\n",
    "# import faiss\n",
    "# import numpy as np\n",
    "\n",
    "with open(\"chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks_data = json.load(f)\n",
    "\n",
    "# Convert each JSON entry back into a LangChain Document\n",
    "chunked_docs = [\n",
    "    Document(page_content=chunk[\"text\"], metadata=chunk[\"metadata\"])\n",
    "    for chunk in chunks_data\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(chunked_docs)} chunks from chunks.json\")\n",
    "\n",
    "# --- Safe embedding wrapper to prevent AttributeError on dict inputs ---\n",
    "class SafeHuggingFaceEmbeddings(HuggingFaceEmbeddings):\n",
    "    def embed_documents(self, texts):\n",
    "        # Coerce all inputs to strings before encoding\n",
    "        clean_texts = [str(t) if not isinstance(t, str) else t for t in texts]\n",
    "        return super().embed_documents(clean_texts)\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        if not isinstance(text, str):\n",
    "            text = str(text)\n",
    "        return super().embed_query(text)\n",
    "\n",
    "# Step 1: Initialize normalized embeddings\n",
    "embedding_model = SafeHuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  # ensures unit-length vectors\n",
    ")\n",
    "\n",
    "# Step 2: Generate embeddings for the documents\n",
    "# --- STEP 3: Generate embeddings manually (optional diagnostic) ---\n",
    "document_embeddings = embedding_model.embed_documents(\n",
    "    [doc.page_content for doc in chunked_docs]\n",
    ")\n",
    "# # Convert to NumPy array (from list of lists) for compatibility with faiss\n",
    "# document_embeddings = np.array(document_embeddings, dtype=\"float32\")\n",
    "\n",
    "# # Normalize the embeddings (force L2 normalization, in case it's not done automatically)\n",
    "# faiss.normalize_L2(document_embeddings)  # This ensures embeddings are unit-normalized\n",
    "\n",
    "# # Step 3: Create FAISS index with Inner Product (cosine similarity)\n",
    "# embedding_dim = document_embeddings.shape[1]\n",
    "# index = faiss.IndexFlatIP(embedding_dim)  # IndexFlatIP for cosine similarity via inner product\n",
    "\n",
    "# # Add the normalized embeddings to the index\n",
    "# index.add(document_embeddings)\n",
    "\n",
    "\n",
    "# # Step 4: Build the FAISS vectorstore (without embedding_model parameter)\n",
    "# vectorstore = FAISS.from_documents(\n",
    "#     chunked_docs,  # Your documents\n",
    "#     embedding_model,\n",
    "#     distance_strategy=DistanceStrategy.COSINE  # Ensures cosine similarity (IP)\n",
    "# )\n",
    "\n",
    "# # Confirm the FAISS index type and metric type\n",
    "# print(f\"FAISS Index Type: {type(index)}\")  # Should be <class 'faiss.IndexFlatIP'>\n",
    "# print(f\"FAISS Metric Type: {index.metric_type}\")  # Should print '1' f4\n",
    "\n",
    "# --- STEP 4: Check the norms of the embeddings ---\n",
    "norms = np.linalg.norm(E, axis=1)\n",
    "print(f\"Min norm: {norms.min()}, Max norm: {norms.max()}, Mean norm: {norms.mean()}\")\n",
    "# If the max norm is greater than 1.0, it means the embeddings are not normalized properly.\n",
    "\n",
    "# --- STEP 5: Normalize embeddings (if they were not already unit-normalized) ---\n",
    "faiss.normalize_L2(E)  # Normalize each vector to unit length\n",
    "\n",
    "# --- STEP 6: Create the FAISS index for Inner Product (Cosine Similarity) ---\n",
    "embedding_dim = E.shape[1]\n",
    "ip_index = faiss.IndexFlatIP(embedding_dim)  # Inner product for cosine similarity\n",
    "\n",
    "# Add the normalized embeddings to the index\n",
    "ip_index.add(E)\n",
    "\n",
    "# --- STEP 7: Confirm the FAISS index type and metric type ---\n",
    "print(f\"FAISS Index Type: {type(ip_index)}\")  # Should print <class 'faiss.IndexFlatIP'>\n",
    "print(f\"FAISS Metric Type: {ip_index.metric_type}\")  # Should print '1' for inner product (cosine similarity)\n",
    "\n",
    "# Optional: Check how many embeddings were created and added\n",
    "print(f\"‚úÖ Total embeddings created: {E.shape[0]}\")\n",
    "\n",
    "# --- STEP 8: Build the FAISS vectorstore ---\n",
    "vectorstore = FAISS.from_documents(\n",
    "    chunked_docs,  # Your documents\n",
    "    embedding_model,\n",
    "    distance_strategy=DistanceStrategy.COSINE  # Ensures cosine similarity (IP)\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5097e56e-8b64-48a2-9a59-5fb927e4cfba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FAISS Index Type: <class 'faiss.swigfaiss_avx2.IndexFlatIP'>\n",
      "FAISS Metric Type: 0\n"
     ]
    }
   ],
   "source": [
    "# # Create FAISS vectorstore\n",
    "# vectorstore = FAISS.from_documents(\n",
    "#     chunked_docs,  # Your documents\n",
    "#     embedding_model,\n",
    "#     distance_strategy=DistanceStrategy.COSINE  # Ensures cosine similarity (IP)\n",
    "# )\n",
    "\n",
    "# Confirm the FAISS index type and metric type\n",
    "print(f\"FAISS Index Type: {type(index)}\")  # Should be <class 'faiss.IndexFlatIP'>\n",
    "print(f\"FAISS Metric Type: {index.metric_type}\")  # Should print '1' for IP (cosine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae7f972c-a004-4827-81ad-2b4bba6fba03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw embeddings norms: Min=0.9999999403953552, Max=1.0000001192092896, Mean=1.0\n",
      "Normalized embeddings norms: Min=0.9999999403953552, Max=1.0000001192092896, Mean=1.0\n",
      "Raw embedding sample: [ 0.07793531 -0.00717798  0.00692454  0.03471662  0.00192955]\n",
      "Normalized embedding sample: [ 0.07793531 -0.00717798  0.00692454  0.03471662  0.00192955]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# Assume `embedding_model` has already been used to create embeddings for your documents\n",
    "raw_embeddings = embedding_model.embed_documents([doc.page_content for doc in chunked_docs])\n",
    "\n",
    "# Convert the list of embeddings to a NumPy array (for compatibility with FAISS)\n",
    "raw_embeddings = np.array(raw_embeddings, dtype=\"float32\")\n",
    "\n",
    "# Check the norm of raw embeddings (L2 norm)\n",
    "raw_norms = np.linalg.norm(raw_embeddings, axis=1)\n",
    "print(f\"Raw embeddings norms: Min={raw_norms.min()}, Max={raw_norms.max()}, Mean={raw_norms.mean()}\")\n",
    "\n",
    "# Normalize the embeddings\n",
    "faiss.normalize_L2(raw_embeddings)\n",
    "\n",
    "# Check the norm of normalized embeddings (L2 norm should be 1 for all embeddings)\n",
    "normalized_norms = np.linalg.norm(raw_embeddings, axis=1)\n",
    "print(f\"Normalized embeddings norms: Min={normalized_norms.min()}, Max={normalized_norms.max()}, Mean={normalized_norms.mean()}\")\n",
    "\n",
    "# Optional: You can also print out some raw vs normalized embedding values to compare\n",
    "print(f\"Raw embedding sample: {raw_embeddings[0][:5]}\")  # Show first 5 values of the first embedding\n",
    "print(f\"Normalized embedding sample: {raw_embeddings[0][:5]}\")  # Show first 5 values of the first embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "096a7d32-c77b-4e1a-8889-35aa716d946a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retriever (cosine) created. k = 4, threshold applied in search_with_scores().\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Retriever + calibration (v1) ‚Äî using LangChain DistanceStrategy\n",
    "# ============================\n",
    "\n",
    "# ---- 1) Safe cosine scorer + clamp to [0,1] ----\n",
    "from langchain_community.vectorstores import DistanceStrategy\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Tuple\n",
    "\n",
    "# We will remove the custom scorer and use LangChain's built-in cosine scorer\n",
    "# LangChain will handle the mapping of cosine similarity to [0, 1]\n",
    "\n",
    "# # Ensure the store is COSINE end-to-end\n",
    "# vectorstore.distance_strategy = DistanceStrategy.COSINE\n",
    "\n",
    "# ---- 2) Manual thresholding on the mapped [0,1] scale ----\n",
    "def search_with_scores(\n",
    "    q: str, \n",
    "    k: int = 6, \n",
    "    score_threshold: float = 0.050\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Returns (Document, relevance_0_1) pairs with cosine-based relevance in [0,1],\n",
    "    filtered by score_threshold.\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(q, k=k)  # raw cosine/IP\n",
    "    out: List[Tuple[Document, float]] = []\n",
    "    for doc, raw_ip in results:\n",
    "        rel = (float(raw_ip) + 1.0) / 2.0  # Mapping raw cosine similarity [-1, 1] to [0, 1]\n",
    "        if rel >= score_threshold:\n",
    "            out.append((doc, rel))\n",
    "    return out\n",
    "\n",
    "\n",
    "# ---- 3) Retriever for chains (no built-in thresholding here) ----\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",          # keep it simple; threshold in chain logic\n",
    "    search_kwargs={\"k\": 4}             # tune k as you like; gating is above\n",
    ")\n",
    "print(\"‚úÖ Retriever (cosine) created. k = 4, threshold applied in search_with_scores().\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f5ef603c-be16-482e-8819-d8f30a6fe8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Retriever + calibration (v1) ‚Äî using LangChain DistanceStrategy\n",
    "# ============================\n",
    "\n",
    "# ---- 1) Safe cosine scorer + clamp to [0,1] ----\n",
    "from langchain_community.vectorstores import DistanceStrategy\n",
    "from langchain_core.documents import Document\n",
    "from typing import List, Tuple\n",
    "\n",
    "# We will remove the custom scorer and use LangChain's built-in cosine scorer\n",
    "# LangChain will handle the mapping of cosine similarity to [0, 1]\n",
    "\n",
    "# Ensure the store is COSINE end-to-end\n",
    "vectorstore.distance_strategy = DistanceStrategy.COSINE\n",
    "\n",
    "# ---- 2) Manual thresholding on the mapped [0,1] scale ----\n",
    "def search_with_scores(\n",
    "    q: str, \n",
    "    k: int = 6, \n",
    "    score_threshold: float = 0.050\n",
    ") -> List[Tuple[Document, float]]:\n",
    "    \"\"\"\n",
    "    Returns (Document, relevance_0_1) pairs with cosine-based relevance in [0,1],\n",
    "    filtered by score_threshold.\n",
    "    \"\"\"\n",
    "    results = vectorstore.similarity_search_with_score(q, k=k)  # raw cosine/IP\n",
    "    out: List[Tuple[Document, float]] = []\n",
    "    for doc, raw_ip in results:\n",
    "        rel = (float(raw_ip) + 1.0) / 2.0  # Mapping raw cosine similarity [-1, 1] to [0, 1]\n",
    "        if rel >= score_threshold:\n",
    "            out.append((doc, rel))\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "495ff899-e285-4194-a8d9-44241d40a6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Retriever (cosine) created. k = 4, threshold applied in search_with_scores().\n",
      "Retrieved Documents:\n",
      "Title: Sample Research Paper\n",
      "Title: Sample Research Paper\n",
      "Title: Sample Research Paper\n",
      "Title: Sample Research Paper\n"
     ]
    }
   ],
   "source": [
    "# ---- 3) Retriever for chains (no built-in thresholding here) ----\n",
    "# Use LangChain's default `as_retriever` method to get the retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",          # Use the default similarity-based search\n",
    "    search_kwargs={\"k\": 4}             # Number of top results to return\n",
    ")\n",
    "print(\"‚úÖ Retriever (cosine) created. k = 4, threshold applied in search_with_scores().\")\n",
    "\n",
    "# Test the retriever with a sample query\n",
    "query = \"What is the impact of AI on healthcare?\"\n",
    "\n",
    "# Use the default `similarity_search` method provided by LangChain's retriever\n",
    "retrieved_docs = retriever.invoke(query)  # Retrieve the top k documents\n",
    "\n",
    "# Print the titles of the retrieved documents\n",
    "print(\"Retrieved Documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    print(f\"Title: {doc.metadata.get('title', 'No title')}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bb4a9064-3c26-4013-943c-3aaffc932475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "Title: Sample Research Paper\n",
      "Content Preview: . In Proceedings of the 2 Conference on Empirical Methods in Natural Language Processing, pages 832‚Äì8. ACL, August 2. Rafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam Shazeer, and Yonghui Wu. Exploring the limits of language modeling. arXiv preprint arXiv:1.02, 2. ≈Åukasz Kaiser and Samy Bengio. Can active memory replace attention? In Advances in Neural Information Processing Systems, (NIPS), 2. ≈Åukasz Kaiser and Ilya Sutskever. Neural GPUs learn algorithms\n",
      "\n",
      "Title: Sample Research Paper\n",
      "Content Preview: Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Need Ashish Vaswani‚àó Google Brain avaswani@google.com Noam Shazeer‚àó Google Brain noam@google.com Niki Parmar‚àó Google Research nikip@google.com Jakob Uszkoreit‚àó Google Research usz@google.com Llion Jones‚àó Google Research llion@google.com Aidan N\n",
      "\n",
      "Title: Sample Research Paper\n",
      "Content Preview: . Curran Associates, Inc., 2. Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104‚Äì3, 2. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1.00, 2. Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Sys\n",
      "\n",
      "Title: Sample Research Paper\n",
      "Content Preview: . In terms of computational complexity, self-attention layers are faster than recurrent layers when the sequence 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Retrieved Documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    title = doc.metadata.get('title', 'No title')\n",
    "    content_preview = doc.page_content[:500]  \n",
    "    print(f\"Title: {title}\")\n",
    "    print(f\"Content Preview: {content_preview}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f18af953-50ee-4e69-a125-d14195f1640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved Documents:\n",
      "Chunk ID: 79 | Title: Sample Research Paper\n",
      "Content Preview: . Neural GPUs learn algorithms. In International Conference on Learning Representations (ICLR), 2. Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, and Ko-ray Kavukcuoglu. Neural machine translation in linear time. arXiv preprint arXiv:1.10099v, 2. Yoon Kim, Carl Denton, Luong Hoang, and Alexander M. Rush. Structured attention networks. In International Conference on Learning Representations, 2. Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimiz\n",
      "\n",
      "Chunk ID: 85 | Title: Sample Research Paper\n",
      "Content Preview: . arXiv preprint arXiv:1.06, 2. Nitish Srivastava, Geoffrey E Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdi-nov. Dropout: a simple way to prevent neural networks from overfitting. Journal of Machine Learning Research, 15(1):1929‚Äì1, 2. Sainbayar Sukhbaatar, Arthur Szlam, Jason Weston, and Rob Fergus. End-to-end memory networks. In C. Cortes, N. D. Lawrence, D. D. Lee, M. Sugiyama, and R. Garnett, editors, Advances in Neural Information Processing Systems 2, pages 2440‚Äì2\n",
      "\n",
      "Chunk ID: 86 | Title: Sample Research Paper\n",
      "Content Preview: . Curran Associates, Inc., 2. Ilya Sutskever, Oriol Vinyals, and Quoc VV Le. Sequence to sequence learning with neural networks. In Advances in Neural Information Processing Systems, pages 3104‚Äì3, 2. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. CoRR, abs/1.00, 2. Vinyals & Kaiser, Koo, Petrov, Sutskever, and Hinton. Grammar as a foreign language. In Advances in Neural Information Processing Sys\n",
      "\n",
      "Chunk ID: 77 | Title: Sample Research Paper\n",
      "Content Preview: . In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770‚Äì7, 2. Sepp Hochreiter, Yoshua Bengio, Paolo Frasconi, and J√ºrgen Schmidhuber. Gradient flow in recurrent nets: the difficulty of learning long-term dependencies, 2. Sepp Hochreiter and J√ºrgen Schmidhuber. Long short-term memory. Neural computation, 9(8):1735‚Äì1, 1. Zhongqiang Huang and Mary Harper. Self-training PCFG grammars with latent annotations across languages\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Simple query to check the chunks\n",
    "query = \"What is machine learning?\"\n",
    "\n",
    "# Retrieve the relevant documents (chunks) using the retriever\n",
    "retrieved_docs = retriever.invoke(query)  # Use the default `invoke()` for retrieval\n",
    "\n",
    "# Print out the results with chunk IDs and content preview\n",
    "print(\"Retrieved Documents:\")\n",
    "for doc in retrieved_docs:\n",
    "    title = doc.metadata.get('title', 'No title')\n",
    "    chunk_id = doc.metadata.get('chunk_id', 'No chunk ID')  # Assuming you added chunk_id to metadata\n",
    "    content_preview = doc.page_content[:500]  # Preview the first 500 characters of content\n",
    "    print(f\"Chunk ID: {chunk_id} | Title: {title}\")\n",
    "    print(f\"Content Preview: {content_preview}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "36fd93e1-ecb2-4c0d-9b9d-f8bb4f33376e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using ChatHuggingFace over HuggingFaceEndpoint (conversational)\n",
      "Model loaded: Mistralai/Mistral-7B-Instruct-v0.2\n"
     ]
    }
   ],
   "source": [
    "# ============================\n",
    "# Cell 3 ‚Äî HF Chat endpoint (v1)\n",
    "# ============================\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n",
    "\n",
    "HF_TOKEN = os.getenv(\"HUGGINGFACEHUB_API_TOKEN\")\n",
    "hf_model_id = \"Mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "\n",
    "# Base endpoint configured for the conversational task\n",
    "_base = HuggingFaceEndpoint(\n",
    "    repo_id=hf_model_id,\n",
    "    task=\"conversational\",          # provider supports this task for this model\n",
    "    huggingfacehub_api_token=HF_TOKEN,\n",
    "    temperature=0.1,\n",
    "    max_new_tokens=300,\n",
    "    top_p=0.9,\n",
    "    return_full_text=False,\n",
    ")\n",
    "\n",
    "# Wrap as a ChatModel so LangChain uses chat flow (not text_generation)\n",
    "llm = ChatHuggingFace(llm=_base)\n",
    "\n",
    "print(\"‚úÖ Using ChatHuggingFace over HuggingFaceEndpoint (conversational)\")\n",
    "print(f\"Model loaded: {hf_model_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "502bbe42-3322-4e6d-a037-3decbec5f08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Strict RAG chain ready (cosine, manual threshold). If nothing qualifies, it returns ‚ÄúI don‚Äôt know based on the provided document.‚Äù\n"
     ]
    }
   ],
   "source": [
    "# =======================================================\n",
    "# Cell 4 ‚Äî Retrieval pipeline (v1 LCEL) with strict gate\n",
    "# =======================================================\n",
    "from operator import itemgetter\n",
    "from typing import List\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough, RunnableBranch\n",
    "\n",
    "# Safety checks\n",
    "if llm is None:\n",
    "    raise RuntimeError(\"LLM not created. Run Cell 3 first.\")\n",
    "if retriever is None:\n",
    "    raise RuntimeError(\"Retriever not created. Build/load FAISS first.\")\n",
    "\n",
    "# ---- Strict, PDF-bounded system rules (use the agreed refusal text) ----\n",
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are a QA assistant for research papers.\n",
    "You must answer strictly and only from the provided context chunks.\n",
    "If the answer is not contained in the context, reply: 'I don‚Äôt know based on the provided document.'\n",
    "Cite page numbers if present in metadata (e.g., 'p. 12').\n",
    "Do not use external knowledge. Do not speculate.\n",
    "Always provide the answer in a concise, factual style, directly quoting or paraphrasing from the context.\n",
    "If multiple chunks contain relevant information, combine them but clearly cite page numbers for each part.\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", SYSTEM_PROMPT),\n",
    "    (\"human\", \"Question: {question}\\n\\nContext:\\n{context}\\n\\nAnswer:\")\n",
    "])\n",
    "\n",
    "# ---- Context formatter with file + page for later citation in the answer ----\n",
    "def format_context(docs: List[Document]) -> str:\n",
    "    blocks = []\n",
    "    for d in docs:\n",
    "        src = d.metadata.get(\"source\", \"\")\n",
    "        page = d.metadata.get(\"page\", \"\")\n",
    "        head = f\"[source: {src}, p. {page}]\" if page != \"\" else f\"[source: {src}]\"\n",
    "        blocks.append(head + \"\\n\" + d.page_content)\n",
    "    return \"\\n\\n---\\n\\n\".join(blocks)\n",
    "\n",
    "# ---- Normalizer (keeps you robust across different LLM client return shapes) ----\n",
    "def normalize_llm_output(x):\n",
    "    if hasattr(x, \"content\"):              # AIMessage\n",
    "        return x.content\n",
    "    if isinstance(x, dict):                # {'generated_text': ...} / {'text': ...}\n",
    "        for k in (\"generated_text\", \"text\", \"answer\", \"content\"):\n",
    "            if k in x and isinstance(x[k], str):\n",
    "                return x[k]\n",
    "        if \"generated_text\" in x and isinstance(x[\"generated_text\"], list):\n",
    "            first = x[\"generated_text\"][0]\n",
    "            if isinstance(first, dict) and \"content\" in first:\n",
    "                return first[\"content\"]\n",
    "        return str(x)\n",
    "    if isinstance(x, list) and x:\n",
    "        first = x[0]\n",
    "        if hasattr(first, \"text\"):\n",
    "            return first.text\n",
    "        if isinstance(first, str):\n",
    "            return first\n",
    "    return str(x)\n",
    "\n",
    "# ---- Gate config (manual thresholding via search_with_scores) ----\n",
    "K = 8\n",
    "SCORE_THRESHOLD = 0.50  # adjust after calibration\n",
    "\n",
    "def pack(inputs):\n",
    "    q = inputs[\"question\"]\n",
    "    # Use manual thresholding on correctly mapped cosine scores\n",
    "    picked = search_with_scores(q, k=K, score_threshold=SCORE_THRESHOLD)  # List[(Document, rel_0_1)]\n",
    "    docs_only = [d for d, rel in picked]\n",
    "    refuse = (len(docs_only) == 0) or all(rel < 0.50 for _, rel in picked)  # strict: no qualified context -> refuse\n",
    "    ctx = format_context(docs_only) if not refuse else \"\"\n",
    "    return {\"question\": q, \"context\": ctx, \"refuse\": refuse}\n",
    "\n",
    "def refuse_text(_):\n",
    "    return \"I don‚Äôt know based on the provided document.\"\n",
    "\n",
    "# ---- LCEL chain: branch BEFORE calling the LLM ----\n",
    "qa_chain = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "    | RunnableLambda(pack)\n",
    "    | RunnableBranch(\n",
    "        (lambda x: x[\"refuse\"], RunnableLambda(refuse_text)),\n",
    "        (prompt | llm | RunnableLambda(normalize_llm_output))\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Strict RAG chain ready (cosine, manual threshold). If nothing qualifies, it returns ‚ÄúI don‚Äôt know based on the provided document.‚Äù\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "907d794b-758e-4ecf-b0fa-150dcd0ea985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The authors of the paper are: Mitchell P Marcus, Mary Ann Marcinkiewicz, Beatrice Santorini, David McClosky, Eugene Charniak, Mark Johnson, Ankur Parikh, Oscar T√§ckstr√∂m, Dipanjan Das, Jakob Uszkoreit, Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N, Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan, Aaron van den Oord, Alex Graves, Ko-ray Kavukcuoglu, Romain Paulus, Caiming Xiong, Richard Socher, Slav Petrov, Leon Barrett, Dan Klein, Ofir Press, Lior Wolf, Denny Britz, Anna Goldie, Minh-Thang Luong, Quoc V. Le, Jianpeng Cheng, Li Dong, and Mirella Lapata. (Citation needed for each author and their respective works)\n",
      "\n",
      "I don‚Äôt know who the authors are for the references numbered 4, 2, and 22 mentioned on p. 1.\n"
     ]
    }
   ],
   "source": [
    "response = qa_chain.invoke({\"question\": \"who are the authors of this paper?\"})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6b02f00d-76fe-4e7a-ab16-086a970765b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß© Irrelevant question test:\n",
      " I don‚Äôt know based on the provided document. The context does not mention the capital city of France.\n"
     ]
    }
   ],
   "source": [
    "test_q = \"What is the capital of France?\"   # not in your PDF\n",
    "print(\"üß© Irrelevant question test:\")\n",
    "print(qa_chain.invoke({\"question\": test_q}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880e1e1-2053-44ba-ae6c-c6e86b4342b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Assignment",
   "language": "python",
   "name": "assignment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
